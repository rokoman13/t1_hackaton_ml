from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import pandas as pd
import sqlite3
import numpy as np
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
import re
import time
import os
import glob
import json
from datetime import datetime
import logging
import functools
from functools import lru_cache

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ==================== MODELS ====================
class QuestionRequest(BaseModel):
    question: str
    top_k: int = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤

class FeedbackRequest(BaseModel):
    question: str
    answer: str
    category: str
    subcategory: str
    rating: int

# ==================== AI SYSTEM ====================
class SupportAI:
    def __init__(self, api_token: str, base_url: str = "https://llm.t1v.scibox.tech/v1"):
        self.api_token = api_token
        self.base_url = base_url
        self.chat_client = OpenAI(api_key=api_token, base_url=base_url)
        self.embed_client = OpenAI(api_key=api_token, base_url=base_url)
        self.conn = None
        self.categories = []
        self.subcategories = []  # –í—Å–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –±–∞–∑—ã
        self.db_file = None
        self._category_embeddings_cache = None
        self._subcategory_embeddings_cache = None
    
    def find_database_file(self) -> bool:
        """–ü–æ–∏—Å–∫ —Ñ–∞–π–ª–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (.db) –∏–ª–∏ CSV –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã"""
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º .db —Ñ–∞–π–ª—ã
        db_files = glob.glob("*.db")
        if db_files:
            self.db_file = db_files[0]
            logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {self.db_file}")
            return True
        
        # –ï—Å–ª–∏ .db –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º CSV —Ñ–∞–π–ª—ã
        csv_files = glob.glob("*.csv")
        for csv_file in csv_files:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª —Å —Ñ–∏–¥–±–µ–∫–æ–º
            if "feedback" in csv_file.lower():
                continue
            logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω CSV —Ñ–∞–π–ª: {csv_file}. –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            if self.create_new_database_from_csv(csv_file):
                return True
        
        logger.warning("üìÅ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
    
    def create_new_database_from_csv(self, csv_path: str) -> bool:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞
        """
        try:
            logger.info(f"üÜï –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV: {csv_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            self.knowledge_df = pd.read_csv(csv_path)
            self.knowledge_df = self.knowledge_df.dropna(subset=['–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è'])
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ['–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞', '–®–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç']
            missing_columns = [col for col in required_columns if col not in self.knowledge_df.columns]
            if missing_columns:
                logger.error(f"‚ùå –í CSV –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
            empty_subcategories = self.knowledge_df['–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].isna().sum()
            if empty_subcategories > 0:
                logger.error(f"‚ùå –ù–∞–π–¥–µ–Ω–æ {empty_subcategories} –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
                return False
            
            # –ß–∏—Å—Ç–∏–º –æ—Ç NaN –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            self.knowledge_df = self.knowledge_df.dropna(subset=['–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞'])
            
            logger.info(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω. –ó–∞–ø–∏—Å–µ–π: {len(self.knowledge_df)}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –±–∞–∑—ã
            self.categories = self.knowledge_df['–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è'].unique().tolist()
            self.subcategories = self.knowledge_df['–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].unique().tolist()
            
            logger.info(f"üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –±–∞–∑–µ: {len(self.categories)}")
            logger.info(f"üìä –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –±–∞–∑–µ: {len(self.subcategories)}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            category_stats = self.knowledge_df.groupby('–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è')['–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è'].unique()
            for category, subcats in category_stats.items():
                logger.info(f"   üìÇ {category}: {len(subcats)} –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π - {', '.join(subcats)}")
            
            # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è –Ω–æ–≤–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            base_name = os.path.splitext(os.path.basename(csv_path))[0]
            self.db_file = f"{base_name}.db"
            
            # –°–æ–∑–¥–∞–µ–º SQLite –±–∞–∑—É
            self.conn = sqlite3.connect(self.db_file, check_same_thread=False)
            self.knowledge_df.to_sql('knowledge_base', self.conn, if_exists='replace', index=False)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            try:
                self.conn.execute('ALTER TABLE knowledge_base ADD COLUMN embedding BLOB')
            except:
                pass  # –ö–æ–ª–æ–Ω–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            self.conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_category ON knowledge_base ("–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è", "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è")
            ''')
            self.conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_question ON knowledge_base ("–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞")
            ''')
            
            self.conn.commit()
            logger.info(f"‚úÖ –ù–æ–≤–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {self.db_file}")
            
            # –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–û –≤—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ–π –±–∞–∑—ã
            logger.info("üîÑ –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
            self.precompute_embeddings()
            
            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
            self._precache_categories()
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def precompute_embeddings(self):
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –±–∞–∑–µ
        """
        try:
            cursor = self.conn.execute('SELECT rowid, "–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞" FROM knowledge_base')
            items = cursor.fetchall()

            total_items = len(items)
            logger.info(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {total_items} –≤–æ–ø—Ä–æ—Å–æ–≤...")

            success_count = 0
            skip_count = 0

            for i, (rowid, question) in enumerate(items):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç–º–±–µ–¥–¥–∏–Ω–≥
                cursor_check = self.conn.execute('SELECT embedding FROM knowledge_base WHERE rowid = ?', (rowid,))
                existing_embedding = cursor_check.fetchone()

                if existing_embedding and existing_embedding[0] is not None:
                    skip_count += 1
                    continue

                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
                embedding = self.get_embeddings(question)
                if embedding:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –º–∞—Å—Å–∏–≤ float64
                    embedding_array = np.array(embedding, dtype=np.float64)
                    embedding_blob = embedding_array.tobytes()
                    self.conn.execute(
                        'UPDATE knowledge_base SET embedding = ? WHERE rowid = ?',
                        (embedding_blob, rowid)
                    )
                    success_count += 1

                # –ö–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø–∏—Å–µ–π
                if (i + 1) % 10 == 0:
                    self.conn.commit()
                    logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{total_items} –≤–æ–ø—Ä–æ—Å–æ–≤ (—É—Å–ø–µ—à–Ω–æ: {success_count}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skip_count})")

                #time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API

            self.conn.commit()
            logger.info(f"üéØ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—Å–ø–µ—à–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
            logger.info(f"üìä –ò—Ç–æ–≥–∏: —É—Å–ø–µ—à–Ω–æ {success_count}, –ø—Ä–æ–ø—É—â–µ–Ω–æ {skip_count}, –≤—Å–µ–≥–æ {total_items}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    
    def setup_database(self) -> bool:
        if self.find_database_file():
            try:
                # –ï—Å–ª–∏ –±–∞–∑–∞ —É–∂–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ –≤ create_new_database_from_csv, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º
                if self.conn is None:
                    self.conn = sqlite3.connect(self.db_file, check_same_thread=False)
                
                cursor = self.conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='knowledge_base'")
                if cursor.fetchone() is None:
                    logger.error("‚ùå –í –±–∞–∑–µ –Ω–µ—Ç —Ç–∞–±–ª–∏—Ü—ã knowledge_base")
                    return False
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã
                if not self.categories:
                    cursor = self.conn.execute('SELECT DISTINCT "–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è" FROM knowledge_base')
                    self.categories = [row[0] for row in cursor.fetchall() if row[0]]
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã
                if not self.subcategories:
                    cursor = self.conn.execute('SELECT DISTINCT "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è" FROM knowledge_base')
                    self.subcategories = [row[0] for row in cursor.fetchall() if row[0]]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                if not self.categories:
                    logger.error("‚ùå –í –±–∞–∑–µ –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    return False
                
                if not self.subcategories:
                    logger.error("‚ùå –í –±–∞–∑–µ –Ω–µ—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    return False
                
                logger.info(f"‚úÖ –ë–∞–∑–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(self.categories)}, –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(self.subcategories)}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                logger.info("üìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
                for category in self.categories:
                    cursor = self.conn.execute(
                        'SELECT DISTINCT "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è" FROM knowledge_base WHERE "–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è" = ?',
                        (category,)
                    )
                    subcats_for_category = [row[0] for row in cursor.fetchall() if row[0]]
                    logger.info(f"   üìÇ {category}: {len(subcats_for_category)} –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    if subcats_for_category:
                        logger.info(f"      üìÅ {', '.join(subcats_for_category)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
                cursor = self.conn.execute('SELECT COUNT(*) FROM knowledge_base WHERE embedding IS NOT NULL')
                embedded_count = cursor.fetchone()[0]
                cursor = self.conn.execute('SELECT COUNT(*) FROM knowledge_base')
                total_count = cursor.fetchone()[0]
                logger.info(f"üìä –ó–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {total_count}, —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {embedded_count}")
                
                # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
                if self._category_embeddings_cache is None:
                    self._precache_categories()
                
                return True
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
                return False
        return False
    
    def _precache_categories(self):
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        logger.info("üëÅüêΩüëÅ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        try:
            # –ö—ç—à–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if self.categories:
                category_embeddings = []
                for category in self.categories:
                    embedding = self.get_embeddings(category)
                    if embedding:
                        category_embeddings.append(embedding)
                    else:
                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
                        category_embeddings.append([])  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É

                if len(category_embeddings) == len(self.categories):
                    self._category_embeddings_cache = category_embeddings
                    logger.info(f"‚úÖ –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ {len(self.categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    self._category_embeddings_cache = None

            # –ö—ç—à–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if self.subcategories:
                subcategory_embeddings = []
                for subcategory in self.subcategories:
                    embedding = self.get_embeddings(subcategory)
                    if embedding:
                        subcategory_embeddings.append(embedding)
                    else:
                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {subcategory}")
                        subcategory_embeddings.append([])  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É

                if len(subcategory_embeddings) == len(self.subcategories):
                    self._subcategory_embeddings_cache = subcategory_embeddings
                    logger.info(f"‚úÖ –ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ {len(self.subcategories)} –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    self._subcategory_embeddings_cache = None

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            self._category_embeddings_cache = None
            self._subcategory_embeddings_cache = None

        
    
    @lru_cache(maxsize=1000)
    def get_embeddings(self, text: str) -> Optional[List[float]]:
        """–ö—ç—à–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        try:
            response = self.embed_client.embeddings.create(model="bge-m3", input=[text])
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return None
    
    def find_top_subcategories(self, entities_text: str, main_category: str, top_k: int = 2) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-N –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –±–∞–∑—ã
            cursor = self.conn.execute(
                'SELECT DISTINCT "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è" FROM knowledge_base WHERE "–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è" = ?',
                (main_category,)
            )
            available_subcategories = [row[0] for row in cursor.fetchall() if row[0]]

            if not available_subcategories:
                return []

            # –ï—Å–ª–∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –º–µ–Ω—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ top_k, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ
            if len(available_subcategories) <= top_k:
                return [{'subcategory': subcat, 'similarity': 1.0} for subcat in available_subcategories]

            # –ï—Å–ª–∏ –∫—ç—à –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ top_k
            if not self._subcategory_embeddings_cache:
                return [{'subcategory': subcat, 'similarity': 1.0} for subcat in available_subcategories[:top_k]]

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_embedding = self.get_embeddings(entities_text)
            if not entities_embedding:
                return [{'subcategory': subcat, 'similarity': 1.0} for subcat in available_subcategories[:top_k]]

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
            entities_embedding_array = np.array(entities_embedding, dtype=np.float64).reshape(1, -1)

            # –ù–∞—Ö–æ–¥–∏–º —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
            subcategory_similarities = []
            for subcat in available_subcategories:
                if subcat in self.subcategories:
                    idx = self.subcategories.index(subcat)
                    if idx < len(self._subcategory_embeddings_cache):
                        subcat_embedding = self._subcategory_embeddings_cache[idx]
                        if not subcat_embedding:
                            continue

                        subcat_embedding_array = np.array(subcat_embedding, dtype=np.float64).reshape(1, -1)

                        if entities_embedding_array.shape[1] != subcat_embedding_array.shape[1]:
                            continue

                        similarity = cosine_similarity(entities_embedding_array, subcat_embedding_array)[0][0]
                        subcategory_similarities.append({'subcategory': subcat, 'similarity': similarity})

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-N
            subcategory_similarities.sort(key=lambda x: x['similarity'], reverse=True)
            return subcategory_similarities[:top_k]

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ç–æ–ø –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return []
        
    def _select_best_category_with_llm(self, user_question: str, entities_text: str, top_candidates: List[Dict]) -> Dict[str, Any]:
        """–í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –ø–∞—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è-–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è —á–µ—Ä–µ–∑ LLM"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è LLM
            candidates_text = ""
            for i, candidate in enumerate(top_candidates, 1):
                category = candidate['category']
                similarity = candidate['similarity']
                subcategories = ", ".join([f"{sub['subcategory']} ({sub['similarity']:.3f})" for sub in candidate['subcategories']])
                candidates_text += f"{i}. –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category} (—Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.3f})\n   –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {subcategories}\n"

            prompt = f"""
    –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_question}"
    –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities_text}

    –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π:
    {candidates_text}

    –í—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø–∞—Ä—É –ö–ê–¢–ï–ì–û–†–ò–Ø-–ü–û–î–ö–ê–¢–ï–ì–û–†–ò–Ø –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.
    –ë—É–¥—å –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω: –≤ –≤–æ–ø—Ä–æ—Å–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞, –≤—ã–≥–ª—è–¥—è—â–µ–µ –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä —Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä: —Ö–∞–ª–≤–∞, –º–æ—Ä–µ, –Ω–∞ –≤—Å—ë –ø—Ä–æ –≤—Å—ë –∏ –ø–æ–¥–æ–±–Ω–æ–µ)
    –ë—É–¥—å –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω: —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ –∫–∞–∫–æ–π-–ª–∏–±–æ –∫–æ–Ω–∫—Ç—Ä–µ—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç –±–∞–Ω–∫–∞
    –í–µ—Ä–Ω–∏ –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:

    {{
        "selected_category": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
        "selected_subcategory": "–Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏", 
        "confidence": 0.95,
    }}
    """

            response = self.chat_client.chat.completions.create(
                model="qwen2.5-72b-h100",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=500
            )

            result_text = response.choices[0].message.content.strip()
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'category': result['selected_category'],
                    'subcategory': result['selected_subcategory'],
                    'confidence': result['confidence'],
                    'reasoning': "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ + –ø—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ LLM 2 —Ç–æ–ø-–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤"
                }

            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
            first_candidate = top_candidates[0]
            return {
                'category': first_candidate['category'],
                'subcategory': first_candidate['subcategories'][0]['subcategory'],
                'confidence': first_candidate['similarity'],
                'reasoning': "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏"
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ LLM: {e}")
            first_candidate = top_candidates[0]
            return {
                'category': first_candidate['category'],
                'subcategory': first_candidate['subcategories'][0]['subcategory'],
                'confidence': first_candidate['similarity'],
                'reasoning': "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ LLM (–æ–ø—è—Ç—å qwen –æ—Ç–≤–∞–ª–∏–ª—Å—è ü§°)"
            }
    
    def _extract_entities_with_llm(self, user_question: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ LLM"""
        try:
            prompt = f"""
            –ò–∑–≤–ª–µ–∫–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞ –±–∞–Ω–∫–∞.
            –ë—É–¥—å –≤–Ω–∏–º–∞—Ç–µ–ª–µ–Ω: –≤ –≤–æ–ø—Ä–æ—Å–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞, –≤—ã–≥–ª—è–¥—è—â–µ–µ –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä —Å–ª–æ–≤, –≤–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ—Ä—ã:
            –ö–∞—Ä—Ç–∞ "–º–æ—Ä–µ"
            –ö–∞—Ä—Ç–∞ "—Ñ–æ—Ä—Å–∞–∂"
            –∫–∞—Ä—Ç–∞ "–∫–æ–º–ø–ª–∏–º–µ–Ω—Ç"
            –∫–∞—Ä—Ç–∞ "—Å–∏–≥–Ω–∞—Ç—É—Ä–∞"
            –∫–∞—Ä—Ç–∞ "–ø–ª–∞—Ç–æ–Ω"
            –∫–∞—Ä—Ç–∞ "–∫—Å—Ç–∞—Ç–∏"
            –∫–∞—Ä—Ç–∞ "—á–µ—Ä–µ–ø–∞—Ö–∞"
            –∫–∞—Ä—Ç–∞ "–Ω–∞ –≤—Å–µ –ø—Ä–æ –≤—Å–µ"
            –∫–∞—Ä—Ç–∞ "–¥–∞–ª—å—à–µ –º–µ–Ω—å—à–µ"
            —ç–∫—Å–ø—Ä–µ—Å—Å-–∫—Ä–µ–¥–∏—Ç "–Ω–∞ —Ä–æ–¥–Ω–∞–µ"
            —Ä—É–±–ª—ë–≤—ã–µ "—Å—É–ø–µ—Ä—Å–µ–º—å" –∏ —Ç–æ–º—É –ø–æ–¥–æ–±–Ω–æ–µ
            –ù–ï –ò–ì–ù–û–†–ò–†–£–ô –ø–æ–¥–æ–±–Ω—ã–µ —Å–ª–æ–≤–∞
            –í–û–ü–†–û–°: "{user_question}"
            
            –í–ï–†–ù–ò –¢–û–õ–¨–ö–û JSON:
            {{
                "entities": {{
                    "products": ["—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"],
                    "actions": ["—Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π"], 
                    "problems": ["—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º"],
                    "objects": ["—Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤"]
                }}
            }}
            """
            
            response = self.chat_client.chat.completions.create(
                model="qwen2.5-72b-h100",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=400
            )
            
            result_text = response.choices[0].message.content.strip()
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            
            entities = {"products": [], "actions": [], "problems": [], "objects": []}
            if json_match:
                entities_data = json.loads(json_match.group())
                entities = entities_data.get("entities", entities)
            
            return entities
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return {"products": [], "actions": [], "problems": [], "objects": []}

    def extract_entities_and_classify(self, user_question: str) -> Optional[Dict[str, Any]]:
        logger.info("üîç –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
        start_time = time.time()

        try:
            # –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
            entities = self._extract_entities_with_llm(user_question)
            logger.info(f"üè∑Ô∏è –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities}")

            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¢–û–õ–¨–ö–û –∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_text = " ".join([
                " ".join(entities.get('products', [])),
                " ".join(entities.get('actions', [])), 
                " ".join(entities.get('problems', [])),
                " ".join(entities.get('objects', []))
            ]).strip()

            if not entities_text:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞")
                return None

            logger.info(f"üéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º: {entities_text}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫—ç—à –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω
            if not self._category_embeddings_cache or not self.categories:
                logger.error("‚ùå –ö—ç—à –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
                return None

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_embedding = self.get_embeddings(entities_text)
            if not entities_embedding:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å—É—â–Ω–æ—Å—Ç–µ–π")
                return None

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
            entities_embedding_array = np.array(entities_embedding, dtype=np.float64).reshape(1, -1)

            # –ù–∞—Ö–æ–¥–∏–º –¢–û–ü-3 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
            category_similarities = []
            for i, category in enumerate(self.categories):
                if i >= len(self._category_embeddings_cache):
                    continue

                category_embedding = self._category_embeddings_cache[i]
                if not category_embedding:
                    continue

                category_embedding_array = np.array(category_embedding, dtype=np.float64).reshape(1, -1)

                if entities_embedding_array.shape[1] != category_embedding_array.shape[1]:
                    continue

                similarity = cosine_similarity(entities_embedding_array, category_embedding_array)[0][0]
                category_similarities.append((category, similarity))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
            category_similarities.sort(key=lambda x: x[1], reverse=True)
            top_categories = category_similarities[:3]

            logger.info(f"üéØ –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {[(cat, round(sim, 3)) for cat, sim in top_categories]}")

            # –ù–∞—Ö–æ–¥–∏–º –¢–û–ü-3 –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            top_category_subcategories = []
            for category, similarity in top_categories:
                # –ó–î–ï–°–¨ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º find_top_subcategories –≤–º–µ—Å—Ç–æ find_best_subcategory
                subcategories = self.find_top_subcategories(entities_text, category, top_k=3)
                top_category_subcategories.append({
                    'category': category,
                    'similarity': similarity,
                    'subcategories': subcategories
                })

            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –ø–∞—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è-–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è —á–µ—Ä–µ–∑ LLM
            best_pair = self._select_best_category_with_llm(user_question, entities_text, top_category_subcategories)

            result = {
                "entities": entities,
                "classification": {
                    "main_category": best_pair['category'],
                    "subcategory": best_pair['subcategory'],
                    "confidence": best_pair['confidence'],
                    "reasoning": best_pair['reasoning']
                },
                "top_candidates": top_category_subcategories
            }

            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f}—Å: {best_pair['category']} ‚Üí {best_pair['subcategory']}")
            result['timing'] = {'classification': round(elapsed_time, 2)}
            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º: {e}")
            return None
        
    def semantic_search(self, user_question: str, entities_result: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
        logger.info(f"üîç –≠—Ç–∞–ø 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (top_k={top_k})")
        start_time = time.time()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        entities_text = ""
        if entities_result and 'entities' in entities_result:
            entities = entities_result['entities']
            entities_text = " ".join([
                " ".join(entities.get('products', [])),
                " ".join(entities.get('actions', [])),
                " ".join(entities.get('problems', [])),
                " ".join(entities.get('objects', []))
            ]).strip()

        if not entities_text:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å")
            entities_text = user_question

        logger.info(f"üîé –ü–æ–∏—Å–∫ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º: {entities_text}")

        try:
            entities_embedding = self.get_embeddings(entities_text)
            if not entities_embedding:
                return []

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
            entities_embedding_array = np.array(entities_embedding, dtype=np.float64).reshape(1, -1)

            cursor = self.conn.execute('''
                SELECT rowid, "–û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è", "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è", "–ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞", "–®–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç", embedding 
                FROM knowledge_base WHERE embedding IS NOT NULL
            ''')
            knowledge_items = cursor.fetchall()
            logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(knowledge_items)} –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")

            similarities = []
            for item in knowledge_items:
                rowid, category, subcategory, example_question, template_answer, embedding_blob = item

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –±–∞–∑—ã
                example_embedding = np.frombuffer(embedding_blob, dtype=np.float64)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                if entities_embedding_array.shape[1] != example_embedding.shape[0]:
                    logger.warning(f"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: —Å—É—â–Ω–æ—Å—Ç–∏ {entities_embedding_array.shape[1]}, –±–∞–∑–∞ {example_embedding.shape[0]}")
                    continue

                similarity = cosine_similarity(entities_embedding_array, [example_embedding])[0][0]

                similarities.append({
                    'rowid': rowid,
                    'category': category,
                    'subcategory': subcategory,
                    'example_question': example_question,
                    'template_answer': template_answer,
                    'similarity': similarity
                })

            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = similarities[:top_k]

            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(top_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º –∑–∞ {elapsed_time:.2f}—Å")

            return top_results
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º: {e}")
            return []
    
    def generate_final_response1(self, user_question: str, entities_result: Dict[str, Any], search_results: List[Dict[str, Any]]) -> str:
        logger.info("üîç –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
        start_time = time.time()
        
        if not search_results:
            logger.warning("‚ö†Ô∏è –ü–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            elapsed_time = time.time() - start_time
            return "‚ùå –í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤."
        
        context_items = "\n".join([
            f"{i+1}. {item['category']}/{item['subcategory']} (—Å—Ö–æ–∂–µ—Å—Ç—å: {item['similarity']:.3f})\n   –í: {item['example_question']}\n   –û: {item['template_answer']}"
            for i, item in enumerate(search_results)
        ])
        
        prompt = f"""
–í–û–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê: "{user_question}"

–ü–û–•–û–ñ–ò–ï –í–û–ü–†–û–°–´ –ò–ó –ë–ê–ó–´:
{context_items}

–°–§–û–†–ú–ò–†–£–ô –û–¢–í–ï–¢ –í –§–û–†–ú–ê–¢–ï:

üéØ –ö–ê–¢–ï–ì–û–†–ò–Ø: [–∫–∞—Ç–µ–≥–æ—Ä–∏—è/–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è]
üí¨ –û–¢–í–ï–¢ –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê: [—è—Å–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç (–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –û–¢–í–ï–¢ –∏–∑ —Å–∞–º–æ–π –±–ª–∏–∑–∫–æ–π –ø–∞—Ä—ã –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –ù–∏—á–µ–≥–æ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π)]
    
        """
        
        try:
            logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç...")
            response = self.chat_client.chat.completions.create(
                model="qwen2.5-72b-h100",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5, #0.3
                max_tokens=1000
            )
            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed_time:.2f}—Å")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            best_match = search_results[0]
            elapsed_time = time.time() - start_time
            return f"""
üéØ –ö–ê–¢–ï–ì–û–†–ò–Ø: {best_match['category']} ‚Üí {best_match['subcategory']}
üí¨ –û–¢–í–ï–¢ –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê:
{best_match['template_answer']}
            """
 
    def generate_final_response(self, user_question: str, entities_result: Dict[str, Any], search_results: List[Dict[str, Any]]) -> str:
        logger.info("üîç –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")
        start_time = time.time()

        if not search_results:
            logger.warning("‚ö†Ô∏è –ü–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            elapsed_time = time.time() - start_time
            return "‚ùå –í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤."

        # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        
        context_items = "\n".join([
            f"{i+1}. {item['category']}/{item['subcategory']} (—Å—Ö–æ–∂–µ—Å—Ç—å: {item['similarity']:.3f})\n   –í: {item['example_question']}\n   –û: {item['template_answer']}"
            for i, item in enumerate(search_results)
        ])
        
        prompt = f"""
    –í–û–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê: "{user_question}"

    –ü–û–•–û–ñ–ò–ï –í–û–ü–†–û–°–´ –ò–ó –ë–ê–ó–´:
    {context_items}

    –°–§–û–†–ú–ò–†–£–ô –û–¢–í–ï–¢ –í –§–û–†–ú–ê–¢–ï:

    üéØ –ö–ê–¢–ï–ì–û–†–ò–Ø: [–∫–∞—Ç–µ–≥–æ—Ä–∏—è/–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è]

    üí¨ –û–¢–í–ï–¢ –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê: [—è—Å–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –æ—Ç–≤–µ—Ç (–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –û–¢–í–ï–¢ –∏–∑ —Å–∞–º–æ–π –±–ª–∏–∑–∫–æ–π –ø–∞—Ä—ã –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π. –ù–∏—á–µ–≥–æ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π)]

    """

        try:
            logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç...")
            response = self.chat_client.chat.completions.create(
                model="qwen2.5-72b-h100",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5, #0.3
                max_tokens=1000  
            )
            elapsed_time = time.time() - start_time
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {elapsed_time:.2f}—Å")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            best_match = search_results[0]
            elapsed_time = time.time() - start_time
            return f"""
    üéØ –ö–ê–¢–ï–ì–û–†–ò–Ø: {best_match['category']} ‚Üí {best_match['subcategory']}

    üí¨ –û–¢–í–ï–¢ –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê:
    {best_match['template_answer']}

    üìä –°–•–û–ñ–ï–°–¢–¨: {best_match['similarity']:.3f}
    """


    def process_customer_query(self, user_question: str, top_k: int = 5) -> Dict[str, Any]:
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: '{user_question}' (top_k={top_k})")
        total_start_time = time.time()
        timing_info = {}
        
        if self.conn is None:
            logger.error("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
            return {'error': '–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞'}
        
        # –≠—Ç–∞–ø 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification_start_time = time.time()
        entities_result = self.extract_entities_and_classify(user_question)
        if not entities_result:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å")
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å: –æ—Ç–≤–∞–ª–∏–ª—Å—è qwen ü§°'}
        
        timing_info['classification'] = round(time.time() - classification_start_time, 2)
        
        # –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫
        search_start_time = time.time()
        search_results = self.semantic_search(user_question, entities_result, top_k=top_k)
        timing_info['search'] = round(time.time() - search_start_time, 2)
        
        # –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        generation_start_time = time.time()
        final_response = self.generate_final_response(user_question, entities_result, search_results)
        timing_info['generation'] = round(time.time() - generation_start_time, 2)
        
        # –û–±—â–µ–µ –≤—Ä–µ–º—è
        timing_info['total'] = round(time.time() - total_start_time, 2)
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {timing_info['total']}—Å")
        
        return {
            'user_question': user_question,
            'entities_result': entities_result,
            'search_results': search_results,
            'assistant_response': final_response,
            'timing': timing_info
        }

# ==================== FASTAPI APP ====================
app = FastAPI(title="Support AI System", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
support_system = None
feedback_df = pd.DataFrame(columns=['timestamp', 'question', 'answer', 'category', 'subcategory', 'rating'])

@app.on_event("startup")
async def startup_event():
    global support_system
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Support AI System...")
    
    API_TOKEN = "sk-x1F57p5wuevcqg5NfOfb7Q"
    support_system = SupportAI(API_TOKEN)
    
    if not support_system.setup_database():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
    else:
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    if os.path.exists("user_feedback.csv"):
        global feedback_df
        feedback_df = pd.read_csv("user_feedback.csv")
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(feedback_df)} —Ñ–∏–¥–±–µ–∫–æ–≤")

@app.get("/")
async def root():
    logger.info("üìä –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –∫ –∫–æ—Ä–Ω–µ–≤–æ–º—É —ç–Ω–¥–ø–æ–∏–Ω—Ç—É")
    return {"message": "Support AI System API", "status": "running"}

@app.post("/process")
async def process_question(request: QuestionRequest):
    logger.info(f"üîß –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É: '{request.question}' (top_k={request.top_k})")
    
    if not support_system:
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        raise HTTPException(status_code=500, detail="–°–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    start_time = time.time()
    result = support_system.process_customer_query(request.question, request.top_k)
    result['processing_time'] = round(time.time() - start_time, 2)
    
    logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time']} —Å–µ–∫")
    return result

@app.post("/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    global feedback_df
    logger.info(f"üëç –ü–æ–ª—É—á–µ–Ω —Ñ–∏–¥–±–µ–∫ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {feedback.category}")
    
    new_feedback = {
        'timestamp': datetime.now().isoformat(),
        'question': feedback.question,
        'answer': feedback.answer,
        'category': feedback.category,
        'subcategory': feedback.subcategory,
        'rating': feedback.rating
    }
    
    feedback_df = pd.concat([feedback_df, pd.DataFrame([new_feedback])], ignore_index=True)
    feedback_df.to_csv("user_feedback.csv", index=False)
    
    logger.info(f"‚úÖ –§–∏–¥–±–µ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –í—Å–µ–≥–æ: {len(feedback_df)}")
    return {"status": "success", "total_feedback": len(feedback_df)}

@app.get("/feedback/stats")
async def get_feedback_stats():
    logger.info("üìà –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–∏–¥–±–µ–∫–∞")
    
    if len(feedback_df) == 0:
        return {"total_feedback": 0, "positive": 0, "negative": 0, "positive_percentage": 0}
    
    total = len(feedback_df)
    positive = len(feedback_df[feedback_df['rating'] == 1])
    negative = len(feedback_df[feedback_df['rating'] == -1])
    
    return {
        "total_feedback": total,
        "positive": positive,
        "negative": negative,
        "positive_percentage": round((positive / total * 100), 1)
    }

if __name__ == "__main__":
    logger.info("üü¢ –ó–∞–ø—É—Å–∫ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ localhost:8222")
    uvicorn.run(app, host="0.0.0.0", port=8222)